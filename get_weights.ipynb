{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# from binarized_modules import  BinarizeLinear,BinarizeConv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/earapidis/Desktop/BinaryNet.pytorch'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/earapidis/Desktop/BinaryNet.pytorch\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.binarized_modules import  BinarizeLinear,BinarizeConv2d, binarized\n",
    "from models.binarized_modules import  Binarize,HingeLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "        \n",
    "#         self.fc1 = BinarizeLinear(784, 512)\n",
    "#         self.bn1 = nn.BatchNorm1d(512)\n",
    "#         self.htanh1 = nn.Hardtanh()\n",
    "\n",
    "#         self.fc2 = BinarizeLinear(512, 64)\n",
    "#         self.bn2 = nn.BatchNorm1d(64)\n",
    "#         self.htanh2 = nn.Hardtanh()\n",
    "\n",
    "        \n",
    "#         self.fc3 =BinarizeLinear(64,10)\n",
    "\n",
    "#         self.drop=nn.Dropout(0.1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(-1, 28*28)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.htanh1(x)\n",
    "\n",
    "#         x = self.drop(x)\n",
    "\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.htanh2(x)\n",
    "        \n",
    "#         x = self.drop(x)\n",
    "\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "#         # return self.logsoftmax(x)\n",
    "\n",
    "# model = Net()\n",
    "# if cuda:\n",
    "#     torch.cuda.set_device(0)\n",
    "#     model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_bnn import Net\n",
    "# class Net(nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "        \n",
    "#         self.fc1 = BinarizeLinear(784, 512)\n",
    "#         self.bn1 = nn.BatchNorm1d(512)\n",
    "#         self.htanh1 = nn.Hardtanh()\n",
    "\n",
    "#         self.fc2 = BinarizeLinear(512, 32)\n",
    "#         self.bn2 = nn.BatchNorm1d(32)\n",
    "#         self.htanh2 = nn.Hardtanh()\n",
    "\n",
    "        \n",
    "#         self.fc3 =BinarizeLinear(32,10)\n",
    "\n",
    "#         self.drop=nn.Dropout(0.1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(-1, 28*28)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.htanh1(x)\n",
    "\n",
    "#         x = self.drop(x)\n",
    "\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.htanh2(x)\n",
    "        \n",
    "#         x = self.drop(x)\n",
    "\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "#         # return self.logsoftmax(x)\n",
    "\n",
    "model = Net()\n",
    "if cuda:\n",
    "    torch.cuda.set_device(0)\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model ):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sim = 4\n",
    "\n",
    "save_dir = os.path.abspath(\"/shares/bulk/earapidis/saved_models\")\n",
    "models_path = os.path.join(save_dir,f\"model_{model_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = os.path.join(models_path,f\"epoch_7.pth\")\n",
    "model_path = os.path.join(models_path,f\"best.pth\")\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "if cuda:\n",
    "    torch.cuda.set_device(0)\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of Net(\n",
       "  (fc1): BinarizeLinear(in_features=784, out_features=512, bias=True)\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (htanh1): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "  (fc2): BinarizeLinear(in_features=512, out_features=32, bias=True)\n",
       "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (htanh2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "  (fc3): BinarizeLinear(in_features=32, out_features=10, bias=True)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       ")>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0002, Accuracy: 9597/10000 (96%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(95.9700)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1 BinarizeLinear(in_features=784, out_features=512, bias=True)\n",
      "htanh1 Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "fc2 BinarizeLinear(in_features=512, out_features=32, bias=True)\n",
      "htanh2 Hardtanh(min_val=-1.0, max_val=1.0)\n",
      "fc3 BinarizeLinear(in_features=32, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "activations = defaultdict(list)\n",
    "\n",
    "\n",
    "# Hook function to capture activations\n",
    "def hook_fn_fc(module, input, output,activations,name):\n",
    "    activations[name].append(output.clone().detach().cpu())\n",
    "    # Append the activations of the current batch\n",
    "    # activations_fc3.append(output.clone().detach().cpu())\n",
    "\n",
    "# target_layers = [\"htanh1\",\"htanh2\",\"fc2\",\"fc3\",\"fc1\"]\n",
    "target_layers = [\"htanh1\",\"htanh2\",\"fc2\",\"fc3\",\"fc1\"]\n",
    "hooks = []\n",
    "for name, module in model.named_modules():\n",
    "    if name in target_layers:\n",
    "        hook = module.register_forward_hook(lambda module,input,output,name=name : hook_fn_fc(module,input,output,activations,name=name))\n",
    "        hooks.append(hook)\n",
    "        print(name,module)\n",
    "\n",
    "targets = []\n",
    "predictions = []\n",
    "# Process the dataset in batches\n",
    "correct = 0\n",
    "binarized_inputs = []\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        predictions.append(pred.detach().cpu())\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        # break\n",
    "        targets.append(target.cpu())\n",
    "        copy_data = data.clone().detach().cpu()\n",
    "        copy_data=copy_data.view(test_batch_size,28*28)\n",
    "\n",
    "        bin_data = binarized(copy_data)\n",
    "        \n",
    "        activations[\"bin_input\"].append(bin_data)\n",
    "        # binarized_inputs.\n",
    "\n",
    "# Remove the hook after processing\n",
    "for hook in hooks:\n",
    "    hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fc1', 'htanh1', 'fc2', 'htanh2', 'fc3', 'bin_input'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 10])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations[\"fc3\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module fc1, batches : 10, batch size : 1000, one row size: 512\n",
      "module htanh1, batches : 10, batch size : 1000, one row size: 512\n",
      "module fc2, batches : 10, batch size : 1000, one row size: 32\n",
      "module htanh2, batches : 10, batch size : 1000, one row size: 32\n",
      "module fc3, batches : 10, batch size : 1000, one row size: 10\n",
      "module bin_input, batches : 10, batch size : 1000, one row size: 784\n"
     ]
    }
   ],
   "source": [
    "for key in activations.keys():\n",
    "    print(f\"module {key}, batches : {len(activations[key])}, batch size : {activations[key][0].shape[0]}, one row size: {activations[key][0].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 784])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations[\"bin_input\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/shares/bulk/earapidis/sim_data/model_4\n"
     ]
    }
   ],
   "source": [
    "save_sim_data = os.path.abspath(\"/shares/bulk/earapidis/sim_data\")\n",
    "save_sim_data = os.path.join(save_sim_data,f\"model_{model_sim}\")\n",
    "print(save_sim_data)\n",
    "os.makedirs(save_sim_data,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bn1.running_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_num(layer_name):\n",
    "    layer_num=\"\"\n",
    "    if layer_name.endswith(\"1\"):\n",
    "        layer_num = \"layer_1\"\n",
    "    if layer_name.endswith(\"2\"):\n",
    "        layer_num = \"layer_2\"\n",
    "    if layer_name.endswith(\"3\"):\n",
    "        layer_num = \"layer_3\"\n",
    "    return layer_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'layer_1': {'fc': 'fc1', 'bn': 'bn1', 'activation': 'htanh1'},\n",
       "             'layer_2': {'fc': 'fc2', 'bn': 'bn2', 'activation': 'htanh2'},\n",
       "             'layer_3': {'fc': 'fc3'}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = defaultdict(dict)\n",
    "for name,layer in model.named_modules():\n",
    "    foo = \"\"\n",
    "    layer_num = \"\"\n",
    "    if name.startswith(\"fc\"):\n",
    "        foo = \"fc\"\n",
    "    elif name.startswith(\"bn\"):\n",
    "        foo = \"bn\"\n",
    "    elif name.startswith(\"htanh\"):\n",
    "        foo = \"activation\"\n",
    "    \n",
    "    layer_num = get_layer_num(name)\n",
    "    \n",
    "    if foo!=\"\" and layer_num!=\"\":\n",
    "        names[layer_num][foo]=name\n",
    "names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_1': {'fc': 'fc1', 'bn': 'bn1', 'activation': 'htanh1'},\n",
       " 'layer_2': {'fc': 'fc2', 'bn': 'bn2', 'activation': 'htanh2'},\n",
       " 'layer_3': {'fc': 'fc3'},\n",
       " 'path': '/shares/bulk/earapidis/saved_models/model_4/best.pth'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict = dict(names)\n",
    "model_dict[\"path\"] = model_path\n",
    "model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/shares/bulk/earapidis/sim_data/model_4/fc1\n",
      "torch.Size([10000, 512])\n",
      "/shares/bulk/earapidis/sim_data/model_4/htanh1\n",
      "torch.Size([10000, 512])\n",
      "/shares/bulk/earapidis/sim_data/model_4/fc2\n",
      "torch.Size([10000, 32])\n",
      "/shares/bulk/earapidis/sim_data/model_4/htanh2\n",
      "torch.Size([10000, 32])\n",
      "/shares/bulk/earapidis/sim_data/model_4/fc3\n",
      "torch.Size([10000, 10])\n",
      "/shares/bulk/earapidis/sim_data/model_4/bin_input\n",
      "torch.Size([10000, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'layer_1': {'fc1': {'output': '/shares/bulk/earapidis/sim_data/model_4/fc1/binary_data.pt'},\n",
       "              'htanh1': {'output': '/shares/bulk/earapidis/sim_data/model_4/htanh1/binary_data.pt'},\n",
       "              'bin_input': {'input': '/shares/bulk/earapidis/sim_data/model_4/bin_input/binary_data.pt'}},\n",
       "             'layer_2': {'fc2': {'output': '/shares/bulk/earapidis/sim_data/model_4/fc2/binary_data.pt'},\n",
       "              'htanh2': {'output': '/shares/bulk/earapidis/sim_data/model_4/htanh2/binary_data.pt'}},\n",
       "             'layer_3': {'fc3': {'output': '/shares/bulk/earapidis/sim_data/model_4/fc3/binary_data.pt'}}})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_data_dict = defaultdict(dict)\n",
    "\n",
    "for layer_name,values in activations.items():\n",
    "    if layer_name!=\"bin_input\":\n",
    "        layer_path = os.path.join(save_sim_data,layer_name)\n",
    "        print(layer_path)\n",
    "        os.makedirs(layer_path,exist_ok=True)\n",
    "\n",
    "        if \"fc\" in layer_name:\n",
    "            binary_data = torch.stack(values, dim=0)\n",
    "        else:\n",
    "            binary_data = binarized(torch.stack(values, dim=0))\n",
    "\n",
    "        layer_num=get_layer_num(layer_name)\n",
    "        \n",
    "        binary_data = binary_data.view(-1,binary_data.shape[-1])\n",
    "        path = os.path.join(layer_path,\"binary_data.pt\")\n",
    "        sim_data_dict[layer_num][layer_name]={\"output\":path}\n",
    "        torch.save(binary_data,path)\n",
    "        print(binary_data.shape)\n",
    "    else:\n",
    "        binary_data = activations[\"bin_input\"]\n",
    "        binary_data = torch.stack(values, dim=0)\n",
    "        binary_data = binary_data.view(-1,binary_data.shape[-1])\n",
    "        layer_path = os.path.join(save_sim_data,\"bin_input\")\n",
    "        print(layer_path)\n",
    "        print(binary_data.shape)\n",
    "\n",
    "        os.makedirs(layer_path,exist_ok=True)\n",
    "\n",
    "        path = os.path.join(layer_path,\"binary_data.pt\")\n",
    "        sim_data_dict[\"layer_1\"][\"bin_input\"]={\"input\":path}\n",
    "\n",
    "    # module_layer = getattr(model, layer_name)\n",
    "    # weights = binarized(module_layer.weight.detach().cpu())  # Weights tensor\n",
    "    # biases = module_layer.bias.detach().cpu() if module_layer.bias is not None else None  # Bias tensor\n",
    "    # print(weights.shape)\n",
    "    # print(biases.shape)\n",
    "    # break\n",
    "sim_data_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1 weights: torch.Size([512, 784])\n",
      "fc1 bias : torch.Size([512])\n",
      "fc2 weights: torch.Size([32, 512])\n",
      "fc2 bias : torch.Size([32])\n",
      "fc3 weights: torch.Size([10, 32])\n",
      "fc3 bias : torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for layer_name in activations.keys():\n",
    "    if layer_name!=\"bin_input\":\n",
    "        layer = getattr(model, layer_name)\n",
    "        layer_path = os.path.join(save_sim_data,layer_name)\n",
    "        if hasattr(layer,\"weight\"):\n",
    "            weights = binarized(layer.weight.detach().cpu())  # Weights tensor\n",
    "            path = os.path.join(layer_path,\"weights.pt\")\n",
    "\n",
    "            torch.save(weights,path)\n",
    "            print(f\"{layer_name} weights: {weights.shape}\")\n",
    "\n",
    "            layer_num = get_layer_num(layer_name)\n",
    "            if layer_num!=\"\":\n",
    "                sim_data_dict[layer_num][layer_name][\"weights\"]=path\n",
    "\n",
    "        if hasattr(layer,\"bias\"):\n",
    "            if layer.bias is not None:\n",
    "                bias = layer.bias.detach().cpu() if layer.bias is not None else None  # Bias tensor\n",
    "                path = os.path.join(layer_path,\"bias.pt\")\n",
    "                sim_data_dict[layer_num][layer_name][\"bias\"]=path\n",
    "\n",
    "                torch.save(bias,path)\n",
    "                print(layer_name,f\"bias : {bias.shape}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data_dict[\"path\"] = model_dict[\"path\"]\n",
    "sim_data_dict = dict(sim_data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = torch.stack(predictions, dim=0)\n",
    "predictions = predictions.squeeze(-1)\n",
    "predictions = predictions.view(-1)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = torch.stack(targets,dim=0)\n",
    "targets = targets.view(-1)\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(save_sim_data,\"targets.pt\")\n",
    "sim_data_dict[\"targets\"] = path\n",
    "torch.save(targets,path)\n",
    "\n",
    "path = os.path.join(save_sim_data,\"predictions.pt\")\n",
    "sim_data_dict[\"predictions\"] = path\n",
    "torch.save(predictions,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "htanh2 = torch.load(os.path.join(save_sim_data,\"htanh2\",\"binary_data.pt\"))\n",
    "htanh1 = torch.load(os.path.join(save_sim_data,\"htanh1\",\"binary_data.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 7688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_input = htanh1[index]\n",
    "one_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_input = one_input.unsqueeze(0)\n",
    "\n",
    "one_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "model.cpu()\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        one_input= Variable(one_input)\n",
    "        output_fc2 = model.fc2(one_input)\n",
    "        output_bn2 = model.bn2(output_fc2)\n",
    "        output_htanh2 = model.htanh2(output_bn2)\n",
    "b_output_htanh2 = binarized(output_htanh2)\n",
    "b_output_htanh2 = b_output_htanh2.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_htanh2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000e+00,  7.1902e-01,  1.0000e+00, -1.0000e+00, -6.5736e-01,\n",
       "         -1.5793e-03,  7.1636e-03, -1.2026e-02,  9.8564e-01,  6.6669e-04,\n",
       "          5.9626e-01, -2.6584e-01,  1.0000e+00,  9.7218e-03, -1.0000e+00,\n",
       "         -1.0000e+00, -4.8500e-02, -8.3804e-04, -3.0977e-01,  8.3901e-01,\n",
       "         -5.9843e-04, -1.0000e+00, -1.0000e+00, -9.1583e-04, -2.8063e-03,\n",
       "         -1.8865e-01,  3.7032e-01,  7.4953e-01,  1.7832e-01, -2.4491e-03,\n",
       "          4.4013e-01,  7.6972e-01]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_htanh2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "htanh2_hook = htanh2[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(b_output_htanh2,htanh2_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): BinarizeLinear(in_features=784, out_features=512, bias=True)\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (htanh1): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "  (fc2): BinarizeLinear(in_features=512, out_features=32, bias=True)\n",
       "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (htanh2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "  (fc3): BinarizeLinear(in_features=32, out_features=10, bias=True)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('model_structure.yml', 'w') as outfile:\n",
    "    yaml.dump(model_dict, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_1': {'fc1': {'output': '/shares/bulk/earapidis/sim_data/model_4/fc1/binary_data.pt',\n",
       "   'weights': '/shares/bulk/earapidis/sim_data/model_4/fc1/weights.pt',\n",
       "   'bias': '/shares/bulk/earapidis/sim_data/model_4/fc1/bias.pt'},\n",
       "  'htanh1': {'output': '/shares/bulk/earapidis/sim_data/model_4/htanh1/binary_data.pt'},\n",
       "  'bin_input': {'input': '/shares/bulk/earapidis/sim_data/model_4/bin_input/binary_data.pt'}},\n",
       " 'layer_2': {'fc2': {'output': '/shares/bulk/earapidis/sim_data/model_4/fc2/binary_data.pt',\n",
       "   'weights': '/shares/bulk/earapidis/sim_data/model_4/fc2/weights.pt',\n",
       "   'bias': '/shares/bulk/earapidis/sim_data/model_4/fc2/bias.pt'},\n",
       "  'htanh2': {'output': '/shares/bulk/earapidis/sim_data/model_4/htanh2/binary_data.pt'}},\n",
       " 'layer_3': {'fc3': {'output': '/shares/bulk/earapidis/sim_data/model_4/fc3/binary_data.pt',\n",
       "   'weights': '/shares/bulk/earapidis/sim_data/model_4/fc3/weights.pt',\n",
       "   'bias': '/shares/bulk/earapidis/sim_data/model_4/fc3/bias.pt'}},\n",
       " 'path': '/shares/bulk/earapidis/saved_models/model_4/best.pth',\n",
       " 'targets': '/shares/bulk/earapidis/sim_data/model_4/targets.pt',\n",
       " 'predictions': '/shares/bulk/earapidis/sim_data/model_4/predictions.pt'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data_dict = dict(sim_data_dict)\n",
    "with open('sim_data_structure.yml', 'w') as outfile:\n",
    "    yaml.dump(sim_data_dict, outfile, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_1': {'activation': 'htanh1', 'bn': 'bn1', 'fc': 'fc1'},\n",
       " 'layer_2': {'activation': 'htanh2', 'bn': 'bn2', 'fc': 'fc2'},\n",
       " 'layer_3': {'fc': 'fc3'},\n",
       " 'path': '/shares/bulk/earapidis/saved_models/model_4/best.pth'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/home/earapidis/Desktop/BinaryNet.pytorch/model_structure.yml\", 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_1': {'bin_input': {'input': '/shares/bulk/earapidis/sim_data/model_4/bin_input/binary_data.pt'},\n",
       "  'fc1': {'bias': '/shares/bulk/earapidis/sim_data/model_4/fc1/bias.pt',\n",
       "   'output': '/shares/bulk/earapidis/sim_data/model_4/fc1/binary_data.pt',\n",
       "   'weights': '/shares/bulk/earapidis/sim_data/model_4/fc1/weights.pt'},\n",
       "  'htanh1': {'output': '/shares/bulk/earapidis/sim_data/model_4/htanh1/binary_data.pt'}},\n",
       " 'layer_2': {'fc2': {'bias': '/shares/bulk/earapidis/sim_data/model_4/fc2/bias.pt',\n",
       "   'output': '/shares/bulk/earapidis/sim_data/model_4/fc2/binary_data.pt',\n",
       "   'weights': '/shares/bulk/earapidis/sim_data/model_4/fc2/weights.pt'},\n",
       "  'htanh2': {'output': '/shares/bulk/earapidis/sim_data/model_4/htanh2/binary_data.pt'}},\n",
       " 'layer_3': {'fc3': {'bias': '/shares/bulk/earapidis/sim_data/model_4/fc3/bias.pt',\n",
       "   'output': '/shares/bulk/earapidis/sim_data/model_4/fc3/binary_data.pt',\n",
       "   'weights': '/shares/bulk/earapidis/sim_data/model_4/fc3/weights.pt'}},\n",
       " 'path': '/shares/bulk/earapidis/saved_models/model_4/best.pth',\n",
       " 'predictions': '/shares/bulk/earapidis/sim_data/model_4/predictions.pt',\n",
       " 'targets': '/shares/bulk/earapidis/sim_data/model_4/targets.pt'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/home/earapidis/Desktop/BinaryNet.pytorch/sim_data_structure.yml\", 'r') as file:\n",
    "    sim_data = yaml.safe_load(file)\n",
    "sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin_input\n",
      "input\n",
      "/shares/bulk/earapidis/sim_data/model_4/bin_input/binary_data.pt\n",
      "binary_data\n",
      "/shares/bulk/earapidis/sim_data/model_4/bin_input\n",
      "/shares/bulk/earapidis/sim_data/model_4/bin_input/binary_data.h5\n",
      "\n",
      "fc1\n",
      "bias\n",
      "/shares/bulk/earapidis/sim_data/model_4/fc1/bias.pt\n",
      "output\n",
      "/shares/bulk/earapidis/sim_data/model_4/fc1/binary_data.pt\n",
      "binary_data\n",
      "/shares/bulk/earapidis/sim_data/model_4/fc1\n",
      "/shares/bulk/earapidis/sim_data/model_4/fc1/binary_data.h5\n",
      "\n",
      "weights\n",
      "/shares/bulk/earapidis/sim_data/model_4/fc1/weights.pt\n",
      "htanh1\n",
      "output\n",
      "/shares/bulk/earapidis/sim_data/model_4/htanh1/binary_data.pt\n",
      "binary_data\n",
      "/shares/bulk/earapidis/sim_data/model_4/htanh1\n",
      "/shares/bulk/earapidis/sim_data/model_4/htanh1/binary_data.h5\n",
      "\n",
      "fc2\n",
      "bias\n",
      "/shares/bulk/earapidis/sim_data/model_4/fc2/bias.pt\n",
      "output\n",
      "/shares/bulk/earapidis/sim_data/model_4/fc2/binary_data.pt\n",
      "binary_data\n",
      "/shares/bulk/earapidis/sim_data/model_4/fc2\n",
      "/shares/bulk/earapidis/sim_data/model_4/fc2/binary_data.h5\n",
      "\n",
      "weights\n",
      "/shares/bulk/earapidis/sim_data/model_4/fc2/weights.pt\n",
      "htanh2\n",
      "output\n",
      "/shares/bulk/earapidis/sim_data/model_4/htanh2/binary_data.pt\n",
      "binary_data\n",
      "/shares/bulk/earapidis/sim_data/model_4/htanh2\n",
      "/shares/bulk/earapidis/sim_data/model_4/htanh2/binary_data.h5\n",
      "\n",
      "fc3\n",
      "bias\n",
      "/shares/bulk/earapidis/sim_data/model_4/fc3/bias.pt\n",
      "output\n",
      "/shares/bulk/earapidis/sim_data/model_4/fc3/binary_data.pt\n",
      "binary_data\n",
      "/shares/bulk/earapidis/sim_data/model_4/fc3\n",
      "/shares/bulk/earapidis/sim_data/model_4/fc3/binary_data.h5\n",
      "\n",
      "weights\n",
      "/shares/bulk/earapidis/sim_data/model_4/fc3/weights.pt\n",
      "/shares/bulk/earapidis/sim_data/model_4\n",
      "/shares/bulk/earapidis/sim_data/model_4/predictions.h5\n",
      "\n",
      "/shares/bulk/earapidis/sim_data/model_4\n",
      "/shares/bulk/earapidis/sim_data/model_4/targets.h5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import h5py\n",
    "sim_data_h5 = copy.deepcopy(sim_data)\n",
    "\n",
    "for layer, values in sim_data.items():\n",
    "    if \"layer\" in layer:\n",
    "        for name, val in values.items():\n",
    "            print(name)\n",
    "            \n",
    "            for metric, metric_path in val.items():\n",
    "                print(metric)\n",
    "                print(metric_path)\n",
    "                if metric==\"output\" or metric==\"input\":\n",
    "\n",
    "                    file_name = os.path.split(metric_path)[-1].split(\".\")[0]\n",
    "                    print(file_name)\n",
    "                    par_dir = os.path.dirname(metric_path)\n",
    "                    print(par_dir)\n",
    "                \n",
    "                    h5_path = os.path.join(par_dir,f\"{file_name}.h5\")\n",
    "                    sim_data_h5[layer][name][metric] = h5_path\n",
    "                    print(h5_path)\n",
    "                    print()\n",
    "                    tensor_data = torch.load(metric_path)\n",
    "                    with h5py.File(h5_path, \"w\") as f:\n",
    "                        f.create_dataset(\"my_tensor\", data=tensor_data.numpy())  # Save as NumPy array\n",
    "    if layer==\"predictions\" or layer==\"targets\":\n",
    "        tensor_data = torch.load(values)\n",
    "        par_dir = os.path.dirname(values)\n",
    "        file_name = os.path.split(values)[-1].split(\".\")[0]\n",
    "\n",
    "        print(par_dir)\n",
    "    \n",
    "        h5_path = os.path.join(par_dir,f\"{file_name}.h5\")\n",
    "        sim_data_h5[layer] = h5_path\n",
    "        print(h5_path)\n",
    "        print()\n",
    "        with h5py.File(h5_path, \"w\") as f:\n",
    "            f.create_dataset(\"my_tensor\", data=tensor_data.numpy())  # Save as NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_1': {'bin_input': {'input': '/shares/bulk/earapidis/sim_data/model_4/bin_input/binary_data.h5'},\n",
       "  'fc1': {'bias': '/shares/bulk/earapidis/sim_data/model_4/fc1/bias.pt',\n",
       "   'output': '/shares/bulk/earapidis/sim_data/model_4/fc1/binary_data.h5',\n",
       "   'weights': '/shares/bulk/earapidis/sim_data/model_4/fc1/weights.pt'},\n",
       "  'htanh1': {'output': '/shares/bulk/earapidis/sim_data/model_4/htanh1/binary_data.h5'}},\n",
       " 'layer_2': {'fc2': {'bias': '/shares/bulk/earapidis/sim_data/model_4/fc2/bias.pt',\n",
       "   'output': '/shares/bulk/earapidis/sim_data/model_4/fc2/binary_data.h5',\n",
       "   'weights': '/shares/bulk/earapidis/sim_data/model_4/fc2/weights.pt'},\n",
       "  'htanh2': {'output': '/shares/bulk/earapidis/sim_data/model_4/htanh2/binary_data.h5'}},\n",
       " 'layer_3': {'fc3': {'bias': '/shares/bulk/earapidis/sim_data/model_4/fc3/bias.pt',\n",
       "   'output': '/shares/bulk/earapidis/sim_data/model_4/fc3/binary_data.h5',\n",
       "   'weights': '/shares/bulk/earapidis/sim_data/model_4/fc3/weights.pt'}},\n",
       " 'path': '/shares/bulk/earapidis/saved_models/model_4/best.pth',\n",
       " 'predictions': '/shares/bulk/earapidis/sim_data/model_4/predictions.h5',\n",
       " 'targets': '/shares/bulk/earapidis/sim_data/model_4/targets.h5'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_data_h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sim_data_structure_h5.yml', 'w') as outfile:\n",
    "    yaml.dump(sim_data_h5, outfile, default_flow_style=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
