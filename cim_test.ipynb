{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc865033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from models.binarized_modules import binarized\n",
    "# from binarized_modules import  BinarizeLinear,BinarizeConv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "236bd51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = False\n",
    "# cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62f8a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85599be",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51fbfb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a26c3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d875105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mnist_bnn import Net\n",
    "from models.lenet_5 import BinarizedLeNet5_BN as Net\n",
    "\n",
    "model = Net()\n",
    "if cuda:\n",
    "    torch.cuda.set_device(0)\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3117c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = os.path.join(models_path,f\"epoch_7.pth\")\n",
    "model_idx = 1\n",
    "models_path = os.path.abspath(f\"/home/earapidis/BinarizedNN/saved_models/lenet_5/model_{model_idx}\")\n",
    "model_path = os.path.join(models_path,f\"epoch_15.pth\")\n",
    "# model_path = os.path.join(models_path,f\"best.pth\")\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "if cuda:\n",
    "    torch.cuda.set_device(0)\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "227bd705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinarizedLeNet5_BN(\n",
       "  (conv1): BinarizeConv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (htanh1): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "  (pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (conv2): BinarizeConv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (htanh2): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "  (pool2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (fc1): BinarizeLinear(in_features=256, out_features=120, bias=True)\n",
       "  (bn_fc1): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (htanh3): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "  (fc2): BinarizeLinear(in_features=120, out_features=84, bias=True)\n",
       "  (bn_fc2): BatchNorm1d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (htanh4): Hardtanh(min_val=-1.0, max_val=1.0)\n",
       "  (fc3): BinarizeLinear(in_features=84, out_features=10, bias=True)\n",
       "  (bn_fc3): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc484eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 6, 5, 5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "38c44fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = torch.randn(8, 4, 3, 3)\n",
    "inputs = torch.randn(10, 4, 5, 5)\n",
    "inputs = binarized(inputs)\n",
    "filters = binarized(filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2cc01f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8, 5, 5])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With square kernels and equal stride\n",
    "\n",
    "padding = 1\n",
    "\n",
    "\n",
    "\n",
    "output = F.conv2d(inputs, filters, padding=padding)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8c5f5ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference shape: torch.Size([10, 8, 5, 5])\n",
      "Max abs diff (loops): 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv2d_loops(x, w, padding=0):\n",
    "    N, Cin, H, W     = x.shape\n",
    "    Cout, _, Kh, Kw  = w.shape\n",
    "    Hout = H + 2*padding - Kh + 1\n",
    "    Wout = W + 2*padding - Kw + 1\n",
    "\n",
    "    # Zero-pad input\n",
    "    x_p = torch.zeros((N, Cin, H + 2*padding, W + 2*padding))\n",
    "    x_p[:, :, padding:padding+H, padding:padding+W] = x\n",
    "\n",
    "    y = torch.zeros((N, Cout, Hout, Wout))\n",
    "    for n in range(N):\n",
    "        for co in range(Cout):\n",
    "            for i in range(Hout):\n",
    "                for j in range(Wout):\n",
    "                    acc = 0.0\n",
    "                    for ci in range(Cin):\n",
    "                        acc += torch.sum(x_p[n, ci, i:i+Kh, j:j+Kw] * w[co, ci])\n",
    "                    y[n, co, i, j] = acc\n",
    "    return y\n",
    "\n",
    "ref = F.conv2d(inputs, filters, padding=padding)\n",
    "out_loops  = conv2d_loops(inputs, filters, padding)\n",
    "print(\"Reference shape:\", ref.shape)\n",
    "print(\"Max abs diff (loops):\",  (ref - out_loops).abs().max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "07b817d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 32, 32)\n",
      "(10, 5, 5, 2, 32)\n",
      "[[[[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]]\n"
     ]
    }
   ],
   "source": [
    "def conv2d_tiles(x, w, Num_rows, Num_Columns, padding=0):\n",
    "    N, Cin, H, W     = x.shape\n",
    "    Cout, _, Kh, Kw  = w.shape\n",
    "    Hout = H + 2*padding - Kh + 1\n",
    "    Wout = W + 2*padding - Kw + 1\n",
    "\n",
    "\n",
    "\n",
    "    # Zero-pad input\n",
    "    x_p = torch.zeros((N, Cin, H + 2*padding, W + 2*padding))\n",
    "    x_p[:, :, padding:padding+H, padding:padding+W] = x\n",
    "\n",
    "    num_tiles_rows = int(np.ceil((Kw * Kh*Cin)/ Num_rows))\n",
    "    num_tiles_columns = int(np.ceil(Cout / Num_Columns))\n",
    "\n",
    "    crossbar_weights = np.zeros((num_tiles_rows,num_tiles_columns,Num_rows, Num_Columns))\n",
    "    kernel_size = Kh * Kw\n",
    "    tile_i_idx = 0\n",
    "    tile_j_idx = 0\n",
    "    cin_per_cross = Num_rows // kernel_size \n",
    "\n",
    "    print(crossbar_weights.shape)\n",
    "    for co in range(Cout):\n",
    "        tile_j_idx = co // Num_Columns\n",
    "        for ci in range(Cin):\n",
    "            id = (ci%cin_per_cross)\n",
    "            tile_row_start = id*kernel_size\n",
    "            tile_row_end = (id+1)*kernel_size\n",
    "            tile_i_idx = ci // cin_per_cross\n",
    "            flat_w = w[co, ci].view(-1).numpy()\n",
    "            crossbar_weights[tile_i_idx, tile_j_idx, tile_row_start:tile_row_end, co] = flat_w\n",
    "    \n",
    "    input_vec = np.zeros((N,Hout,Wout,num_tiles_rows,Num_rows))\n",
    "    print(input_vec.shape)\n",
    "    for n in range(N):\n",
    "        for ci in range(Cin):\n",
    "            for i in range(Hout):\n",
    "                for j in range(Wout):\n",
    "                    id = (ci%cin_per_cross)\n",
    "                    tile_row_start = id*kernel_size\n",
    "                    tile_row_end = (id+1)*kernel_size\n",
    "                    tile_i_idx = ci // cin_per_cross\n",
    "                    flat_input = torch.flatten(x_p[n, ci, i:i+Kh, j:j+Kw]).numpy()\n",
    "                    # print(flat_input.shape)\n",
    "                    input_vec[n,i,j,tile_i_idx, tile_row_start:tile_row_end] = flat_input\n",
    "\n",
    "    output = np.zeros((N,Cout,Hout,Wout))\n",
    "\n",
    "    for n in range(N):\n",
    "        for i in range(Hout):\n",
    "            for j in range(Wout):\n",
    "                inp = input_vec[n,i,j,:,:]\n",
    "                # print(inp.shape)\n",
    "                w = crossbar_weights.reshape(num_tiles_rows, Num_rows, Num_Columns)\n",
    "                # print(w.shape)\n",
    "\n",
    "                intermidiate_out = np.zeros((num_tiles_rows, Num_Columns))\n",
    "                for idx, vec in enumerate(inp):\n",
    "                    out = np.dot(vec, w[idx,:,:])\n",
    "                    intermidiate_out[idx,:] = out\n",
    "                # print(intermidiate_out.shape)\n",
    "                cout_outs = np.sum(intermidiate_out, axis=0)\n",
    "                cout = cout_outs[:Cout]\n",
    "                output[n,:,i,j] = cout\n",
    "    return output\n",
    "Num_rows = 32\n",
    "Num_Columns = 32                    \n",
    "out_loops  = conv2d_tiles(inputs, filters,Num_rows,Num_Columns, padding)\n",
    "print(out_loops-ref.numpy())\n",
    "# print(\"Output shape:\", out_loops[0].shape, out_loops[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "df9f8143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4, 5, 5])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
